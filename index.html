
<!DOCTYPE html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108253468-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108253468-1');
</script>
  <meta charset="utf-8">
  
  <title>Zechun Liu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Zechun Liu">
<meta property="og:url" content="/var/www/html/index.html">
<meta property="og:site_name" content="Zechun Liu">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zechun Liu">
  
  
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      
        <a class="navbar-brand" href="">Zechun Liu</a>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="#publications">Publications</a></li>
        
          <li><a class=""
                 href=""></a></li>
        
          <li><a class=""
                 href=""></a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  
</div>

    <div class="main">
      
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">

  

  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
          <b><p><img src="./pictures/1_photo.jpg" style="float:left; margin-right: 20px;margin-top: 5px;  height: 300px;"> 

<p><strong><font color="#4590a3" size="5px">About</font></strong></p>


I'm currently a Staff Research Scientist and Tech Lead at Meta. <br>
		  
Before that I was a visiting scholar in <a href="https://www.cmu.edu/">Carnegie Mellon University</a> from Sept 2019 to Dec 2021, advised by <a href="https://www.ece.cmu.edu/directory/bios/savvides-marios.html">Prof.Marios Savvides</a> and <a href="http://www.cs.cmu.edu/~epxing/">Prof. Eric Xing</a>. <br>

I obtained my Ph.D. degree from <a href="https://www.ust.hk/home">Hong Kong University of Science and Technology</a> in June 2021, supervised by <a href="https://seng.ust.hk/about/people/faculty/kwang-ting-tim-cheng">Prof. Kwang-Ting Tim CHENG </a>. <br>

I interned in <a href="https://research.google/teams/brain/"> Google Brain</a> in 2020, mentored by <a href="https://www.linkedin.com/in/chas-leichner-33571582"> Chas Leichner</a>

Before starting my Ph.D. study in HKUST in Sept. 2016, I obtained my bachelor’s degree from Fudan University in June. 2016. </b>
<br><br>

<p><strong><font color="#4590a3" size="5px">Contact</font></strong></p>
<p><img src="" style="float:left; margin-right: 0px; height: 145px;"> Email: zliubq[at]connect[dot]ust[dot]hk; zechunl[at]andrew[dot]cmu[dot]edu<br>
<a href="https://scholar.google.com/citations?user=lA7ylt4AAAAJ&hl=zh-CN" target="_blank" rel="external">[Google Scholar]</a>&nbsp;|&nbsp; <a href="https://github.com/liuzechun" target="_blank" rel="external">[Github]</a> </p>
<br>


<p><strong><font color="#4590a3" size="5px">Research Interest</font></strong></p>

<p> My research focuses on improving the efficiency and deployability of foundation models through architectural optimization, low-bit quantization, sparsity and etc. Specifically, I am interested in using deep learning to solve the practical problems in the industry such as the limitation of insufficient resources and a trade-off between computation and accuracy. My research focus is mainly on:</p>
<ul>
<li> Large language model pretraining and post-training </li>
<li> Neural architecture design and search </li>
<li> Quantization, pruning and sparsity </li>
<li> Knowledge distillation </li>
<li> Efficient vision-languade model </li>
</ul>
<br>

	    
<p><strong><font color="#4590a3" size="5px">News</font></strong></p> 

<li>[Oct 2025] We will organize two workshops at ICCV 2025. <a href="https://binarynetworks.io/"> [LBQNN] </a> and <a href="https://sites.google.com/view/cogs2025"> [COGS] </a> 
<li>[Sept 2025] We have released Meta's first reasoning LLM: MobileLLM-R1. <a href="https://huggingface.co/collections/facebook/mobilellm-r1-68c4597b104fac45f28f448e"> [model] </a> <a href="https://github.com/facebookresearch/MobileLLM-R1"> [code] </a> <a href="https://arxiv.org/abs/2509.24945"> [paper] </a> </li>
<li>[Sept 2025] I will serve as an <b>Area Chair</b> for ICLR 2026. </li>
<li>[Sept 2025] Two papers accepted to NeurIPS 2025. <a href="https://arxiv.org/abs/2502.02631"> [ParetoQ] </a> and [RDD]</li>
<li>[July 2025]	<b>Invited talk</b> at ICML workshop. <a href="https://ttodlerfm.gitlab.io/"> [TTODLer-FM] </a> </li>
<li>[June 2025] One papers accepted to ICCV 2025. <a href="https://arxiv.org/abs/2411.18933"> [Efficient track anything] </a> </li>	
<li>[May 2025] Three papers accepted to ICML 2025. <a href="https://arxiv.org/abs/2410.10934"> [Agent-as-a-judge] </a> <a href="https://arxiv.org/abs/2410.17434"> [LongVU] </a> <a href="https://arxiv.org/abs/2503.15748"> [PARQ] </a> </li>	
<li>[Apr 2025] <b>Invited talk</b> at ICLR workshop. <a href="https://scope-workshop.github.io/"> [SCOPE] </a> </li>
<li>[Jan 2025] Three papers accepted to ICLR 2025. <a href="https://https://arxiv.org/abs/2405.16406"> [SpinQuant] </a> <a href="https://arxiv.org/abs/2504.19449"> [R-sparse] </a> <a href="https://openreview.net/forum?id=vqbd2OQnGp"> [Param$\delta$ for Direct Mixing] </a> </li>	
<li>[Sept 2024] Our SpinQuant technique provided the quantization support for the Meta Connect Live Demos. <a href="https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/"> [post] </a> </li>	
<li>[Sept 2024] Three papers accepted to EMNLP 2024 — one in the Main Conference, one in Findings, and one in the Industry Track. </li>	
<li>[Sept 2024] We have released <a href="https://github.com/facebookresearch/MobileLLM"> [code] </a> and <a href="https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95"> [model weights] </a> of MobileLLM. </li>	
<li>[July 2024] Two papers accepted to ACL 2024 Findings. </li>	
<li>[June 2024] One papers accepted to Scientific Reports. <a href="https://www.nature.com/articles/s41598-024-64677-2"> [paper] </a>  </li>	
<li>[May 2024] MobileLLM is accepted to ICML 2024. <a href="https://arxiv.org/abs/2502.02631"> [paper] </a>  </li>	
<li>[May 2024] Our survey paper on Vision-Language Models (VLMs) are available <a href="https://arxiv.org/abs/2405.17247"> here</a>. </li>	
<li>[Apr 2024] <b>Invited talk</b> at <a href="https://www.emc2-ai.org/asplos-24"> [EMC2 workshop] </a> in ASPLOS 2024. </li>	
<li>[Mar 2024] <b>Invited talk</b> at TinyML forum.  </li>
<li>[Oct 2023] We released <a href="https://minigpt-v2.github.io/"> [MiniGPT-v2] </a>. </li>	
<li>[Sept 2023] One paper accepted to EMNLP 2023. <a href="https://arxiv.org/abs/2310.16836"> [LLM-FP4] </li>	
<li>[July 2023] <a href="https://arxiv.org/abs/2306.01841/"> Binary and Ternary Natural Language Generation </a> is accepted to ACL 2023 as Oral Persentation . </li>	
<li>[June 2023] One paper accepted to TMLR. <a href="https://arxiv.org/abs/2306.07215"> [paper] </a>  </li>	
<li>[May 2023] One paper accepted to ICML 2023. <a href="https://proceedings.mlr.press/v202/liu23w.html"> [paper] </li>
<li>[May 2023] We released <a href="https://arxiv.org/abs/2305.17888"> [LLM-QAT], the first quantization-aware training solution for LLMs. </li>		
<li>[Sept 2022] One papers accepted to NeurIPS 2022. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/5c1863f711c721648387ac2ef745facb-Paper-Conference.pdf"> [BiT] </a>  </li>	
<li>[July 2022] Two papers accepted to ECCV 2022.</li>	
<li>[May 2022] One paper accepted to ICML 2022.</li>	
<li>[Mar 2022] Two papers accepted to CVPR 2022.</li>	
<li>[Dec 2021] I joined Meta Inc. as a Research Scientise.</li>	
<li>[Dec 2021] Two papers accepted to AAAI 2022.</li>	
<li>[Sept 2021] One paper accepted to TIP 2021.</li>	    
<li>[May 2021] One paper accepted to ICML 2021.</li>	 
<li>[Apr 2021] I co-organized a workshop on CVPR 2021:<a href="https://binarynetworks.io/">Binary Networks for Computer Vision</a></li>
<li>[Mar 2021] One paper accepted to CVPR 2021.</li>
<li>[Jan 2021] One paper accepted to ICLR 2021.</li>
<li>[Dec 2020] One paper accepted to AAAI 2021 and one paper accepted to IJCV 2021.</li>
<li>[July 2020] Two papers accepted to ECCV 2020.</li>
<li>[March 2020] One paper accepted to CVPR 2020.</li>
<li>[Sept 2019] I come to Canegie Mellon University as a visiting scholar.</li>
<br>

<p><strong><font id="publications" color="#4590a3" size="5px">Publications </font></strong></p>
<table border=0 cellpadding=0 width=100% style="border-spacing: 0 6px; line-height:14pt; border: 0px">

<tr style="border-width: 1px">
	<td style="border: 0px;"><img src="./pictures/pubilication_figures/MobileLLM-R1.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Changsheng Zhao*, Ernie Chang*, <b>Zechun Liu</b>*†, Chia-Jung Chang, Wei Wen, Chen Lai, Rick Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, Vikas Chandra
			<br>
		† Corresponding Author and Research Lead <br>
		<a href="https://arxiv.org/abs/2509.24945"><b>MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes</b></a><br>
			<b> <a href="https://arxiv.org/abs/2509.24945" style="color:#EE7F2D;">Paper</a> </b>  &nbsp;|&nbsp;
			<b> <a href="https://huggingface.co/collections/facebook/mobilellm-r1-68c4597b104fac45f28f448e" style="color:#EE7F2D;">Models</a> </b>  &nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/MobileLLM-R1" style="color:#EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/ParetoQ.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			<b>Zechun Liu</b>, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, Lin Xiao, Yuandong Tian, Bilge Soran, Raghuraman Krishnamoorthi, Tijmen Blankevoort, Vikas Chandra
			<br>
		<a href="https://arxiv.org/abs/2502.02631"><b>ParetoQ: Scaling Laws in Extremely Low-Bit LLM Quantization</b></a><br>
			<b> <a href="https://arxiv.org/abs/2502.02631" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/ParetoQ"; style="color: #EE7F2D;">Code</a></b>  &nbsp;|&nbsp;
			<b> <a href="https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95"; style="color: #EE7F2D;">Models</a></b>
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/SpinQuant.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			<b>Zechun Liu</b>*, Changsheng Zhao*, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort
			<br>
		<a href="https://arxiv.org/abs/2405.16406"><b>SpinQuant: LLM Quantization with Learned Rotations</b></a><br>
			<b> <a href="https://arxiv.org/abs/2405.16406" style="color:#EE7F2D;">Paper</a></b>&nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/SpinQuant"; style="color: #EE7F2D;">Code</a></b>  &nbsp;|&nbsp;
			<b> <a href="https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/"; style="color: #EE7F2D;">Post</a></b>  &nbsp;|&nbsp;
			<b> <a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8"; style="color: #EE7F2D;">Models</a></b>
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/MobileLLM.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			<b>Zechun Liu</b>, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra
			<br>
		<a href="#"><b>MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases</b></a><br>
			Forty-first International Conference on Machine Learning, 2024.<br>
			<b> <a href="https://arxiv.org/abs/2402.14905" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/MobileLLM"; style="color: #EE7F2D;">Code</a></b>  &nbsp;|&nbsp;
			<b> <a href="https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95"; style="color: #EE7F2D;">Models</a></b>
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/LLM-QAT.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			<b>Zechun Liu</b>*,Barlas Oguz*, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra
			<br>
		<a href="https://arxiv.org/abs/2305.17888"><b>LLM-QAT: Data-Free Quantization Aware Training for Large Language Models</b></a><br>
			ACL 2023 Findings.<br>
			<b> <a href="https://arxiv.org/abs/2305.17888" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/LLM-QAT"; style="color: #EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/EfficientTAM.jpg.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Yunyang Xiong, Chong Zhou, Xiaoyu Xiang, Lemeng Wu, Chenchen Zhu, <b>Zechun Liu</b>, Saksham Suri, Balakrishnan Varadarajan, Ramya Akula, Forrest Iandola, Raghuraman Krishnamoorthi, Bilge Soran, Vikas Chandra
			<br>
		<a href="https://arxiv.org/abs/2411.18933"><b>Efficient Track Anything</b></a><br>
			ICCV 2024.<br>
			<b> <a href="https://arxiv.org/abs/2411.18933" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/yformer/EfficientTAM"; style="color: #EE7F2D;">Code</a></b>  &nbsp;|&nbsp;
			<b> <a href="https://yformer.github.io/efficient-track-anything/"; style="color: #EE7F2D;">Project Page</a></b>  &nbsp;|&nbsp;
			<b> <a href="https://huggingface.co/yunyangx/efficient-track-anything/tree/main"; style="color: #EE7F2D;">Models</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/longvu.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, <b>Zechun Liu</b>, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, Vikas Chandra
			<br>
		<a href="https://arxiv.org/abs/2410.17434"><b>LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</b></a><br>
			ICML 2024.<br>
			<b> <a href="https://arxiv.org/abs/2410.17434" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/Vision-CAIR/LongVU"; style="color: #EE7F2D;">Code</a></b>  &nbsp;|&nbsp;
			<b> <a href="https://vision-cair.github.io/LongVU/"; style="color: #EE7F2D;">Project Page</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/agent-as-a-judge.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, <b>Zechun Liu</b>, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, Jürgen Schmidhuber
			<br>
		<a href="https://arxiv.org/abs/2410.10934"><b>Agent-as-a-Judge: Evaluate Agents with Agents</b></a><br>
			ICML 2024.<br>
			<b> <a href="https://arxiv.org/abs/2410.10934" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://huggingface.co/DEVAI-benchmark"; style="color: #EE7F2D;">Dataset</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/metauto-ai/agent-as-a-judge"; style="color: #EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/parq.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Lisa Jin, Jianhao Ma, <b>Zechun Liu</b>, Andrey Gromov, Aaron Defazio, Lin Xiao
			<br>
		<a href="https://arxiv.org/abs/2503.15748"><b>PARQ: Piecewise-Affine Regularized Quantization</b></a><br>
			ICML 2024.<br>
			<b> <a href="https://arxiv.org/abs/2503.15748" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/parq"; style="color: #EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/r-sparse.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Zhenyu Zhang, <b>Zechun Liu</b>, Yuandong Tian, Harshit Khaitan, Zhangyang Wang, Steven Li
			<br>
		<a href="https://arxiv.org/abs/2504.19449"><b>R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference</b></a><br>
			ICLR 2024.<br>
			<b> <a href="https://arxiv.org/abs/2504.19449" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/VITA-Group/R-Sparse"; style="color: #EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/llm-fp4.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Shih-yang Liu*, <b>Zechun Liu</b>*, Xijie Huang, Pingcheng Dong, Kwang-Ting Cheng
			<br>
		<a href="https://arxiv.org/abs/2310.16836"><b>LLM-FP4: 4-Bit Floating-Point Quantized Transformers</b></a><br>
			EMNLP 2023.<br>
			<b> <a href="https://arxiv.org/abs/2310.16836" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/nbasyl/LLM-FP4"; style="color: #EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/minigpt-v2.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, <b>Zechun Liu</b>, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny
			<br>
		<a href="https://arxiv.org/abs/2310.09478"><b>MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-Task Learning</b></a><br>
			<b> <a href="https://arxiv.org/abs/2310.09478" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://minigpt-v2.github.io/"; style="color: #EE7F2D;">Project Page</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/TBT.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			<b>Zechun Liu</b>*, Barlas Oguz*, Aasish Pappu, Yangyang Shi, Raghuraman Krishnamoorthi
			<br>
		<a href="https://arxiv.org/abs/2306.01841"><b>Binary and Ternary Natural Language Generation</b></a> &nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/Ternary_Binary_Transformer"; style="color: #EE7F2D;">Code</a></b> 
			ACL 2023 (Oral).<br>
			<b> <a href="https://arxiv.org/abs/2306.01841" style="color:#EE7F2D;">Paper</a></b><br>
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/BiT.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			<b>Zechun Liu</b>*, Barlas Oguz*, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, Yashar Mehdad
			<br>
		<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/5c1863f711c721648387ac2ef745facb-Paper-Conference.pdf"><b>BiT: Robustly Binarized Multi-Distilled Transformer</b></a><br>
			<b> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/5c1863f711c721648387ac2ef745facb-Paper-Conference.pdf" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/facebookresearch/bit"; style="color: #EE7F2D;">Code</a></b> 
			NeurIPS 2022.<br>
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>

<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/OFQ.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			Shih-Yang Liu*, <b>Zechun Liu</b>*, Kwang-Ting Cheng
			<br>
		<a href="https://proceedings.mlr.press/v202/liu23w.html"><b>Oscillation-Free Quantization for Low-Bit Vision Transformers</b></a><br>
			ICML 2023.<br>
			<b> <a href="https://proceedings.mlr.press/v202/liu23w.html" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/nbasyl/OFQ"; style="color: #EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>


<tr style="border-width:1px">
	<td style="border:0px;"><img src="./pictures/pubilication_figures/N2UQ.jpg" style="height:150px;width:200px;margin-top:0px"></td>
	<td style="border:0px;">
		<table style="width:100%;"><tr><td style="width:100%;text-align:left;border:0px;">
		<p>
			<b>Zechun Liu</b>, Kwang-Ting Cheng, Dong Huang, Eric P Xing, Zhiqiang Shen
			<br>
		<a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Nonuniform-to-Uniform_Quantization_Towards_Accurate_Quantization_via_Generalized_Straight-Through_Estimation_CVPR_2022_paper.html">Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation</a><br>
			CVPR 2022.<br>
			<b> <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Nonuniform-to-Uniform_Quantization_Towards_Accurate_Quantization_via_Generalized_Straight-Through_Estimation_CVPR_2022_paper.html" style="color:#EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
			<b> <a href="https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization."; style="color: #EE7F2D;">Code</a></b> 
		</p>
		</td><td style="width:10px;border:0px;"></td></tr></table>
	</td>
</tr>


	
        <tr style="border-width: 1px">
                <td style="border: 0px;"><img src="./pictures/pubilication_figures/ICML2021_AdamBNN.png" style="height: 80px; width: 300px; margin-top: 15px"></td>
                <td style="border: 0px;">
                        <table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
                        <p>
                                <b>Zechun Liu*</b>, Zhiqiang Shen*, Shichao Li, Koen Helwegen, Dong Huang, Kwang-Ting Cheng.
                                <br>
                        <a href="https://arxiv.org/abs/2106.11309"><b>How Do Adam and Training Strategies Help BNNs Optimization</b></a><br>
                                International Conference on Machine Learning (ICML), 2021.<br>
                                <b> <a href="https://github.com/liuzechun/AdamBNN"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
                                <b> <a href="https://arxiv.org/abs/2106.11309"; style="color: #EE7F2D;">Paper</a></b>
                        </p>
                        </td><td style="width: 10px;border: 0px">
                        </td>
                        </tr></table>
                </td>
        </tr>

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./pictures/pubilication_figures/ReActNet.png" style="height: 150px; width: 200px; margin-top: 0px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zechun Liu</b>, Zhiqiang Shen, Marios Savvides, Kwang-Ting Cheng.
				<br>
			<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590137.pdf"><b>ReActNet: Towards Precise Binary NeuralNetwork with Generalized Activation Functions</b></a><br>
				European Conference on Computer Vision (ECCV), 2020.<br>
				<b> <a href="https://github.com/liuzechun/ReActNet"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/2003.03488"; style="color: #EE7F2D;">Paper</a></b><br>
				<a style="color: black;">We achieve <b>65.9%</b> (ResNet-based) and <b>69.5%</b> (MobileNet-based) top-1 accuracy on ImageNet (the new results are slightly higher than those in our original paper after we fix a small loading bug), for the first time, exceeding the benchmarking ResNet-level accuracy (69.3%) while achieving more than 22× reduction in computational complexity.</a>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>


	
	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./pictures/pubilication_figures/spos.png" style="height: 90px; width: 200px; margin-top: -2px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, <b>Zechun Liu</b>, Yichen Wei, Jian Sun.
				<br>
			<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610528.pdf"><b>Single Path One-Shot Neural Architecture Search with Uniform Sampling</b></a><br>
				European Conference on Computer Vision (ECCV), 2020.<br>
				<b> <a href="https://github.com/megvii-model/ShuffleNet-Series/tree/master/OneShot"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/1904.00420"; style="color: #EE7F2D;">Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>


	
	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./pictures/pubilication_figures/metapruning.png" style="height: 110px; width: 120px; margin-top: 0px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zechun Liu</b>, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, Jian Sun.
				<br>
			<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_MetaPruning_Meta_Learning_for_Automatic_Neural_Network_Channel_Pruning_ICCV_2019_paper.pdf"><b>MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning</b></a><br>
				IEEE International Conference on Computer Vision (ICCV), 2019.<br>
				<b> <a href="https://github.com/liuzechun/MetaPruning"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/1903.10258"; style="color: #EE7F2D;">Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>




	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./pictures/pubilication_figures/bireal-eccv.png" style="height: 90px; width: 450px; margin-top: 15px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zechun Liu</b>, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng.
				<br>
			<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/zechun_liu_Bi-Real_Net_Enhancing_ECCV_2018_paper.pdf"><b>Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm</b></a><br>
				European Conference on Computer Vision (ECCV), 2018.<br>
				<b> <a href="https://github.com/liuzechun/Bi-Real-net"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/1808.00278"; style="color: #EE7F2D;">Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>

	

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./pictures/pubilication_figures/bop.png" style="height: 80px; width: 450px; margin-top: 10px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				Koen Helwegen, James Widdicombe, Lukas Geiger, <b>Zechun Liu</b>, Kwang-Ting Cheng, Roeland Nusselder.
				<br>
			<a href="https://papers.nips.cc/paper/8971-latent-weights-do-not-exist-rethinking-binarized-neural-network-optimization.pdf"><b>Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization</b></a><br>
				Advances in Neural Information Processing Systems 32 pre-proceedings (NIPS), 2019.<br>
				<b> <a href="https://github.com/plumerai/rethinking-bnn-optimization"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
				<b> <a href=" https://arxiv.org/abs/1906.02107"; style="color: #EE7F2D;">Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>


</table>

	    
	   
<p><strong><font color="#4590a3" size="5px">Academic Services</font></strong></p>
<ul>
Conference Reviewer <br>
<li> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022 </li>
<li> International Conference on Computer Vision (ICCV), 2021 </li>
<li> European Conference on Computer Vision (ECCV), 2022 </li>
<li> International Conference on Learning Representations (ICLR), 2022 </li>
<li> Adcances in Neural Information Processing Systems (NeurIPS), 2021, 2022 </li>
<li> International Conference on Machine Learning (ICML), 2022 </li>
Journal Reviewer <br>
<li> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022 -- Present </li>
<li> IEEE Transactions on Image Processing (TIP), 2020 -- Present </li>
<li> IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2020 -- Present </li>
</ul>
	
<p><strong><font color="#4590a3" size="5px">Awards and Honors</font></strong></p>
<ul>
<li> SENG Academic Award for Continuing PhD Students 2019-20 </li>
<li> Postgraduate Studentship, HKUST 2016-2020 </li>
<li> Oversea Research Award, HKUST 2019 </li>
<li> Shanghai Outstanding Graduate, Fudan University 2016 </li>
<li> Chinese National Scholarship, Fudan University 2012-2013 & 2013-2014 </li>
</ul>
<br>

<p><strong><font color="#4590a3" size="5px">Teaching Assistant</font></strong></p>
<ul>
<li> 2017.2 - 2016.6, HKUST, ELEC 2300 Computer Organization </li>
<li> 2017.9 - 2018.1, HKUST, ELEC 4010K Machine Learning and Information Processing for Robotic Perception </li>
<li> 2017.2 - 2016.6, HKUST, ELEC 2300 Computer Organization </li>	
</ul>

	    
</div>
   
</div>
</article>

<div align="center">
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=858585&w=300&t=n&d=QLkoiXSxiN0bb8INVm1oB3dAORWmDBpQKl7qOsphiq8&co=ffffff&cmo=91b7f0&cmn=a06ce3'></script>
</div>

    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2019 Zechun Liu. All rights reserved<br>
      (Last update: Dec. 1, 2019.)<br>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>


<script src="script.js"></script>

</body>
</html>

