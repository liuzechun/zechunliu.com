
<!DOCTYPE html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108253468-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108253468-1');
</script>
  <meta charset="utf-8">
  
  <title>Zhiqiang Shen</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Zhiqiang Shen">
<meta property="og:url" content="/var/www/html/index.html">
<meta property="og:site_name" content="Zhiqiang Shen">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zhiqiang Shen">
  
  
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      
        <a class="navbar-brand" href="">Zhiqiang Shen</a>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="#publications">Publications</a></li>
        
          <li><a class=""
                 href=""></a></li>
        
          <li><a class=""
                 href=""></a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  
</div>

    <div class="main">
      
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">

  

  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
    <!--  <p><img src="./me2.JPG" style="float:left; margin-right: 20px; height: 280px;">I am currently a joint-training Ph.D. student (or called visiting scholar) in the IFP (Image Formation and Processing) group of the University of Illinois at Urbana-Champaign, advised by <a href="http://www.ece.illinois.edu/directory/profile/t-huang1">Prof. Thomas S. Huang</a>. I am also a Ph.D. student in the Institute of Intelligent Media Computing at Fudan University, under the supervision of <a href="http://www.yugangjiang.info/">Prof. Yu-Gang Jiang</a> and <a href="https://scholar.google.com/citations?user=DTbhX6oAAAAJ&hl=en">Prof. Xiangyang Xue</a>. From July 2016 to July 2017, I was a research intern at Intel Labs China.</p> -->
          <b><p><img src="./me2.JPG" style="float:left; margin-right: 20px;margin-top: 5px;  height: 300px;">I am currently a post-doctoral researcher in <a href="https://www.cylab.cmu.edu/index.html">CyLab</a>, <a href="https://www.ece.cmu.edu/">ECE Department</a>, <a href="https://www.cmu.edu/">CMU</a>, advised by <a href="https://www.ece.cmu.edu/directory/bios/savvides-marios.html">Prof. Marios Savvides</a>. My research interests span computer vision, machine learning, deep learning, etc.<br><br>
Prior to CMU, I was fortunate to be a joint-training Ph.D student (2017-2019) in <a href="https://illinois.edu/">UIUC</a>/<a href="http://ifp-uiuc.github.io/">IFP group</a>, advised by <a href="http://www.ece.illinois.edu/directory/profile/t-huang1">Prof. Thomas S. Huang</a>.<br><br>
I received my Ph.D from Fudan University working with <a href="http://www.yugangjiang.info/">Prof. Yu-Gang Jiang</a> and <a href="https://scholar.google.com/citations?user=DTbhX6oAAAAJ&hl=en">Prof. Xiangyang Xue</a>. I was a research intern at Intel Labs China and SenseTime Research, collaborating with <a href="https://scholar.google.com/citations?user=7OTD-LEAAAAJ&hl=en">Zhuang Liu</a>, <a href="https://scholar.google.com/citations?user=n44GlFcAAAAJ&hl=en">Jianguo Li</a>, <a href="https://scholar.google.com/citations?user=MKRyHXsAAAAJ&hl=en">Yurong Chen</a> (ILC, 2016-2017), <a href="">Mingyang Huang</a>, <a href="https://scholar.google.com/citations?hl=en&user=mwsxrm4AAAAJ">Jianping Shi</a> (SenseTime Research).</p></b>
<!--<br><br>School of Computer Science<br>Fudan University<br><b>Room 408</b>, Computer Science Building, 825 Zhangheng Road<br>Email: zhiqiangshen13 AT fudan.edu.cn / zhiqiangshen0214 AT gmail.com<br><a href="https://scholar.google.com/citations?user=DGr0fVoAAAAJ&hl=en" target="_blank" rel="external">[Google Scholar]</a>&nbsp;|&nbsp; <a href="https://github.com/szq0214" target="_blank" rel="external">[Github]</a><!-- &nbsp;|&nbsp; <a href="" target="_blank" rel="external">[CV]</a></p>-->
<p><img src="" style="float:left; margin-right: 0px; height: 145px;">Department of Electrical and Computer Engineering<br>Carnegie Mellon University<br>5000 Forbes Avenue, Pittsburgh, PA, 15213<br>Email: zhiqians AT andrew.cmu.edu | zhiqiangshen0214 AT gmail.com <br> <del>shen54 AT illinois.edu</del> | <del>zhiqiangshen13 AT fudan.edu.cn</del><br> <a href="https://scholar.google.com/citations?user=DGr0fVoAAAAJ&hl=en" target="_blank" rel="external">[Google Scholar]</a>&nbsp;|&nbsp; <a href="https://github.com/szq0214" target="_blank" rel="external">[Github]</a><!-- &nbsp;|&nbsp; <a href="" target="_blank" rel="external">[CV]</a>--></p><br>
<p><strong><font color="#4590a3" size="4px">Research Interest</font></strong></p>
<p>My research focuses on the broad area of computer vision and machine learning. Specifically, I am interested in deep learning methods for object detection, fine-grained recognition, image/video captioning, etc. Recently, I focus on</p>
<ul>
<li>Few-shot and zero-shot learning</li>
<li>Designing and training high-efficiency network structure</li>
<li>Video Analysis, including detection, captioning, summarization and prediction</li>
<li>Image Understanding, including visual question answering, captioning, fine-grained recognition and detection</li>
<li>Weakly-supervised/Unsupervised Learning </li>
<li>Low-bit Networks</li>
</ul>
<p><strong><font color="#4590a3" size="4px">News</font></strong></p>
<ul>
<li>[07/14, 2019] <a style="color: red;"><b>New:</b></a> <a href="https://arxiv.org/abs/1905.01744" style="color: black;"><b>INIT dataset</b></a> has been released, please check out our <a href="./projects/INIT/index.html" style="color: red;"><b>Project Page</b></a>. </li>
<li>[07/1, 2019] One paper accepted to <a href="https://bmvc2019.org/" style="color: black;"><b>BMVC 2019</b></a>. </li>
<li>[06/5, 2019] <a style="color: red;"><b>New:</b></a> <a href="https://arxiv.org/abs/1809.09294" style="color: black;"><b>DSOD V2</b></a> accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" style="color: black;"><b>IEEE transactions on pattern analysis and machine intelligence (TPAMI)</b></a>. In this version, we have provided more ablation studies and some preliminary results on exploring the factors of training two-stage detectors from scratch.</li>
<li>[02/26, 2019] One paper accepted to <a href="http://cvpr2019.thecvf.com/" style="color: black;"><b>CVPR 2019</b></a>. </li>
<li>[12/21, 2018] One paper accepted to <a href="https://iclr.cc/" style="color: black;"><b>ICLR 2019</b></a>. </li>
<li>[11/1, 2018] Our paper <a href="https://arxiv.org/abs/1812.02425" style="color: black;"><b>MEAL: Multi-Model Ensemble via Adversarial Learning</b></a> accepted in <b>AAAI 2019</b> as <b style="color: #EE7F2D;">Oral Presentation</b>. Code and models are available at <b> <a href="https://github.com/AaronHeee/MEAL"; style="color: red;">Github</a></b>.</li>
<li>[09/27, 2018] An extended version of <a href="https://arxiv.org/abs/1708.01241" style="color: black;"><b>DSOD</b></a> is available on: <b> <a href="https://arxiv.org/abs/1809.09294"; style="color: black;">arXiv</a></b>.</li>
<li>[07/29, 2018] One paper accepted to <a href="https://eccv2018.org/" style="color: black;"><b>ECCV 2018</b></a>. </li>
<li>[01/12, 2018] I gave an invited talk at the <a href="http://research.baidu.com/institute-of-deep-learning/" style="color: black;"><strong>Baidu IDL</strong></a>, Sunnyvale, CA, USA on the topic of <strong>learning object detectors from scratch</strong>. My talk involved our recent two papers <a href="https://arxiv.org/abs/1708.01241" style="color: black;"><strong>DSOD</strong></a> and <a href="https://arxiv.org/abs/1712.00886" style="color: black;"><b>GRP-DSOD</b></a>. Slides can be downloaded <b><a href="./projects/Talk_slides/Detection_Baidu_IDL.pdf"; style="color: red;">here</a></b> (or <b><a href="https://drive.google.com/open?id=1r950Cmuqefo3ubUZ1jY_IrKzwIDUvWK4"; style="color: black;">Google Drive</a></b>).</li>
<li>[12/22, 2017] We released the <a href="https://github.com/szq0214/MSR-VTT-Challenge" style="color: black;"><strong>code and model</strong></a> for <a href="http://ms-multimedia-challenge.com/2016/challenge" style="color: black;"><b>MSR-VTT Challenge (Video Captioning)</b></a> on <b> <a href="https://github.com/szq0214/MSR-VTT-Challenge"; style="color: red;">Github</a></b>.</li>
<li>[12/04, 2017] Our new paper <a href="https://arxiv.org/abs/1712.00886" style="color: black;"><b>GRP-DSOD</b></a> is available at: <b> <a href="https://arxiv.org/abs/1712.00886"; style="color: red;">arXiv</a></b>. Code and models are available at <b> <a href="https://github.com/szq0214/GRP-DSOD"; style="color: red;">Github</a></b>.</li>
<li>Code and models for <a href="https://arxiv.org/abs/1708.01241" style="color: black;"><b>DSOD</b></a> are available at: <b> <a href="https://github.com/szq0214/DSOD"; style="color: red;">Github</a></b>.</li>
<li>Code and models for <a href="https://arxiv.org/abs/1708.06519" style="color: black;"><b>Network Slimming</b></a> are available at: <b> <a href="https://github.com/liuzhuang13/slimming"; style="color: red;">Github</a></b>.</li>
<li><strong >Two papers accepted to ICCV 2017.</strong></li>
<li><del>Two papers submitted to ICCV 2017.</del></li>
<li>Our paper <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Weakly_Supervised_Dense_CVPR_2017_paper.pdf" style="color: black;"><strong>"Weakly Supervised Dense Video Captioning"</strong></a> accepted to CVPR 2017.</li>
<li>Our paper <a href="https://arxiv.org/pdf/1703.09983.pdf" style="color: black;"><strong>"Iterative Object and Part Transfer for Fine-Grained Recognition"</strong></a> accepted to ICME 2017 as an <strong >oral</strong> presentation.</li>
<li>During my internship, our team won the <strong>2016 Intel China Award (ICA)</strong>, the highest award for team achievement in Intel China.</li>
<li>4th Place (Human Evaluation) and 5th Place (Automatic Evaluation Metrics) Winners at the <a href="http://ms-multimedia-challenge.com/2016/leaderboard">MSR-VTT Challenge</a> (Video Captioning). <b>Code is <a href="https://github.com/szq0214/MSR-VTT-Challenge" style="color: black;"><strong>here</strong></a> </b>.</li>
<!--
<li><button id="pre_news">Past News</button></li>
<li id="pre1" style="display: none"><date>May-2017</date> Test1</li>
<li id="pre2" style="display: none"><date>May-2017</date> Test2</li>
<li id="pre3" style="display: none"><date>July-2016</date> Test3</li>
<li id="pre4" style="display: none"><date>May-2016</date> Test4</li>
<li id="pre5" style="display: none"><date>Mar-2016</date> Test5</li>
-->
</ul>
<p><strong><font id="publications" color="#4590a3" size="4px">Publications </font></strong></p>
<table border=0 cellpadding=0 width=100% style="border-spacing: 0 6px; line-height:14pt; border: 0px">


	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/RA/RA.png" style="height: 55px; width: 200px; margin-top: 25px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				Zhuang Liu, Tinghui Zhou, <b>Zhiqiang Shen</b>, Bingyi Kang, Trevor Darrell.
				<br>
			<a href="https://arxiv.org/abs/1910.09185"><b>Transferable Recognition-Aware Image Processing</b></a><br>
				arXiv preprint arXiv:1910.09185 (2019).<br>
				<b> <a href="https://arxiv.org/abs/1910.09185"; style="color: #EE7F2D;">arXiv Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>


	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/MEAL/update.jpg" style="height: 100px; width: 200px; margin-top: 0px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zhiqiang Shen</b>, Zhankui He, Wanyun Cui, Jiahui Yu, Yutong Zheng, Chenchen Zhu, Marios Savvides.
				<br>
			<a href="https://arxiv.org/abs/1908.08520"><b>Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement</b></a><br>
				Extended version of our <b> <a href="https://arxiv.org/abs/1812.02425"; style="color: red;">MEAL</a></b>. 
				arXiv preprint arXiv:1908.08520 (2019).<br>
				<b> <a href="https://arxiv.org/abs/1908.08520"; style="color: #EE7F2D;">arXiv Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>


	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/MobiNet/MoBiNet.png" style="height: 100px; width: 200px; margin-top: 0px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				Hai Phan, Dang Huynh, Yihui He, Marios Savvides, <b>Zhiqiang Shen</b>.
				<br>
			<a href="https://arxiv.org/abs/1907.12629"><b>MoBiNet: A Mobile Binary Network for Image Classification</b></a><br>
				Tech report. arXiv preprint arXiv:1907.12629 (2019).<br>
				<b> <a href="https://arxiv.org/abs/1907.12629"; style="color: #EE7F2D;">arXiv Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>


	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/GFR-DSOD/GRP-DSOD.png" style="height: 100px; width: 200px; margin-top: 0px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zhiqiang Shen</b>, Honghui Shi, Jiahui Yu, Hai Phan, Rogerio Feris, Liangliang Cao, Ding Liu, Xinchao Wang, Thomas Huang, Marios Savvides.
				<br>
			<a href="./projects/GFR-DSOD/0663.pdf"><b>Improving Object Detection from Scratch via Gated Feature Reuse</b></a><br>
				30th British Machine Vision Conference (BMVC), 2019.<br>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/DSOD_pami/scratch.png" style="height: 100px; width: 200px; margin-top: 2px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zhiqiang Shen</b>, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, Xiangyang Xue.
				<br>
			<a href="https://arxiv.org/abs/1809.09294"><b>Object Detection from Scratch with Deep Supervision</b></a><br>
				IEEE transactions on pattern analysis and machine intelligence (T-PAMI), 2019.<br>
				<!---arXiv preprint:1809.09294.<br>--->
				<b> <a href="https://github.com/szq0214/DSOD"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/1809.09294"; style="color: #EE7F2D;">arXiv Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>
	

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/INIT/I2I.png" style="height: 100px; width: 200px; margin-top: 2px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zhiqiang Shen</b>, Mingyang Huang, Jianping Shi, Xiangyang Xue, Thomas S. Huang.
				<br>
			<a href="https://arxiv.org/abs/1905.01744"><b>Towards Instance-level Image-to-Image Translation</b></a><br>
				IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br>
				<b><a href="./projects/INIT/index.html"; style="color: #EE7F2D;">Project</a></b> &nbsp;|&nbsp;
				<b><a href="https://arxiv.org/abs/1905.01744"; style="color: #EE7F2D;">Paper</a></b> &nbsp;|&nbsp;
				<b><a href="./projects/INIT/index.html"; style="color: #EE7F2D;">Dataset</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr> 

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/MEAL/MEAL.jpg" style="height: 100px; width: 200px; margin-top: 2px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zhiqiang Shen*</b>, Zhankui He*, Xiangyang Xue.
				<br>
			<a href="https://arxiv.org/pdf/1812.02425.pdf"><b>MEAL: Multi-Model Ensemble via Adversarial Learning</b></a><br>
				33rd AAAI Conference on Artificial Intelligence (AAAI), 2019.
				<b style="color: #EE7F2D;">(Oral Presentation)</b><br>
				<b> <a href="https://github.com/AaronHeee/MEAL"; style="color: #EE7F2D;">Code & Models</a></b> &nbsp;|&nbsp;
				<b> <b style="color: black;">Our ResNet-50 (Top-1/5: 21.70%/5.99%) &nbsp; <a href="https://drive.google.com/open?id=1x6SUiPWbqIKtdF_XRtEBQuinRfHUiRvm"; style="color: red;">[PyTorch Model (102.5M)]</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./publication/imgs/ICLR_2019_TRANSFER.png" style="height: 100px; width: 200px; margin-top: 0px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
			     Wanyun Cui, Guangyu Zheng, <b>Zhiqiang Shen</b>, Sihang Jiang, Wei Wang.<br>
			<a href="./publication/ICLR_2019_TRANSFER.pdf"><b>Transfer Learning for Sequences via Learning to Collocate</b></a><br>
				International Conference on Learning Representations (ICLR), 2019.<br>
				<b> <a href="https://openreview.net/forum?id=ByldlhAqYQ"; style="color: #EE7F2D;">OpenReview</a></b> &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/1902.09092"; style="color: #EE7F2D;">arXiv Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>




	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/TS2C/eccv2018.png" style="height: 100px; width: 200px; margin-top: 15px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				Yunchao Wei, <b>Zhiqiang Shen*</b>, Bowen Cheng*, Honghui Shi, Jinjun Xiong, Jiashi Feng, Thomas Huang.
				<br>
			<a href="https://arxiv.org/pdf/1807.04897.pdf"><b>TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection</b></a><br>
				European Conference on Computer Vision (ECCV), 2018.
				(* indicates equal contribution)<br>
				<b> <a href="https://arxiv.org/pdf/1807.04897.pdf"; style="color: #EE7F2D;">arXiv Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/Slimming/Slimming.png" style="height: 60px; margin-top: 17px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				Zhuang Liu, Jianguo Li, <b>Zhiqiang Shen</b>, Gao Huang, Shoumeng Yan, Changshui Zhang.
				<br>
			<a href="https://arxiv.org/pdf/1708.06519.pdf"><b>Learning Efficient Convolutional Networks through Network Slimming</b></a><br>
				Proceedings of 16th IEEE International Conference on Computer Vision (<b>ICCV2017</b>).<br>
				<b> <a href="https://github.com/liuzhuang13/slimming"; style="color: #EE7F2D;">Code & Models</a></b>  &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/pdf/1708.06519.pdf"; style="color: #EE7F2D;">Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>

<!--
	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/GRP-DSOD/GRP-DSOD.png" style="height: 100px; width: 200px; margin-top: 15px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zhiqiang Shen*</b>, Honghui Shi*, Rogerio Feris, Liangliang Cao, Shuicheng Yan, Ding Liu, Xinchao Wang, Xiangyang Xue, Thomas S. Huang.
				<br>
			<a href="https://arxiv.org/abs/1712.00886"><b>Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids</b></a><br>
				arXiv preprint arXiv:1712.00886.<br>
				(* indicates equal contribution)<br>
				<b> <a href="https://github.com/szq0214/GRP-DSOD"; style="color: #EE7F2D;">Code & Models</a></b> &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/1712.00886"; style="color: #EE7F2D;">arXiv Paper</a></b>

			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>
-->

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./projects/DSOD/DSOD.png" style="height: 100px; width: 200px; margin-top: 8px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p>
				<b>Zhiqiang Shen*</b>, Zhuang Liu*, Jianguo Li, Yu-Gang Jiang, Yurong Chen, Xiangyang Xue.
				<br>
			<a href="https://arxiv.org/pdf/1708.01241.pdf"><b>DSOD: Learning Deeply Supervised Object Detectors from Scratch</b></a><br>
				Proceedings of 16th IEEE International Conference on Computer Vision (<b>ICCV2017</b>).<br>
				(* indicates equal contribution)<br>
				<b> <a href="https://github.com/szq0214/DSOD"; style="color: #EE7F2D;">Code & Models</a></b> &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/abs/1708.01241"; style="color: #EE7F2D;">Paper</a></b>

			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>
	

	<tr style="border-width: 1px; border: 0px">
		<td style="border: 0px;"><img src="./DenseVidCap.png" style="height: 100px;"></td>
		<td style="border: 0px">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p><b>Zhiqiang Shen</b>, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue.<br>
			<a href="https://arxiv.org/pdf/1704.01502.pdf"><b>Weakly Supervised Dense Video Captioning</b></a><br>
				Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2017</b>).<br>
				<b><a href="./projects/DenseVidCap/index.html"; style="color: #EE7F2D;">Project</a></b> &nbsp;|&nbsp;
				<b><a href="https://arxiv.org/pdf/1704.01502.pdf"; style="color: #EE7F2D;">Paper</a></b>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>	

	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./Fine-grained_ICME17.png" style="height: 120px;"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px;">
			<p><b>Zhiqiang Shen</b>, Yu-Gang Jiang, Dequan Wang, Xiangyang Xue.<br>
			<a href="https://arxiv.org/pdf/1703.09983.pdf"><b>Iterative Object and Part Transfer for Fine-Grained Recognition</b></a><br>
				Proceedings of 18th IEEE Conference on Multimedia & Expo (<b>ICME2017</b>).<br>
			    <b style="color: #EE7F2D;">Oral Presentation</b> &nbsp;|&nbsp;
				<b> <a href="https://arxiv.org/pdf/1703.09983.pdf"; style="color: #EE7F2D;">Paper</a></b>

			    <!---&nbsp;&#183;&nbsp;
				<a href="https://arxiv.org/pdf/1703.09983.pdf">Paper</a>&nbsp;&#183;&nbsp;
				<a href="">Project Webpage</a>-->
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>	
	
	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./ICCV15.png" style="height: 100px; width: 210px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td style="width: 100%; text-align: left; border: 0px; margin-top: 50px">
			<p>Dequan Wang, <b>Zhiqiang Shen</b>, Jie Shao, Wei Zhang, Xiangyang Xue, Zheng Zhang.<br>
			<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Multiple_Granularity_Descriptors_ICCV_2015_paper.pdf"><b>Multiple granularity descriptors for fine-grained categorization</b></a><br>
				Proceedings of 15th IEEE International Conference on Computer Vision (<b>ICCV2015</b>).<br>
			</p>
			</td><td style="width: 10px;border: 0px">
			</td>
			</tr></table>			
		</td>
	</tr>	
	
	<tr style="border-width: 1px">
		<td style="border: 0px;"><img src="./CVPR15_workshop.png" class="center" style="width: 210px; margin-top: 30px"></td>
		<td style="border: 0px;">
			<table style="width: 100%;"><tr><td class='onecenter' style="width: 100%; text-align: left; border: 0px;">
			<p>Dequan Wang, Tianjun Xiao, <b>Zhiqiang Shen</b>, Xiangyang Xue.<br>
			<a href="./publication/CVPR_2015_Workshop_Learning.pdf"><b>Learning Regions and Descriptors for Fine-grained Recognition</b></a><br>
				Proceedings of 28th IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR2015 FGVC3 Workshop</b>).<br>
			</p>	
			</td><td style="width: 10px; border: 0px">
			</td>
			</tr></table>	
		</td>
	</tr>	

</table>

<p><strong><font color="#4590a3" size="4px">Academic Activities</font></strong></p>
<ul>
<li> Conference reviewer: <b>CVPR 2020</b>, <b>AAAI 2020</b>, <b>ICCV 2019</b>, <b>CVPR 2019</b>, <b>AAAI 2019</b>, <b>CVPR 2018</b>, <b>ACCV 2018</b>, <b>NIPS 2016</b>.</li>
<li> Journal reviewer: <b>IJCV</b>, <b>TIP</b>, <b>TMM</b>, <b>JVCI</b>.</li>
	
</ul>

<p><strong><font color="#4590a3" size="4px">Awards and Honors</font></strong></p>
<ul>
<li>CVPR 2019 Doctoral Consortium travel award. Mentor: <a href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>.</li>
<li>ICLR 2019 travel award, 2019</li>
<li>AAAI 2019 student scholarship award, 2018</li>
<li>ICCV 2017 student volunteer, 2017</li>
<li>Huawei scholarship, 2017</li>
<li>During my internship, our team won the 2016 Intel China Award (ICA), the highest award for team achievement in Intel China, 2016</li>
<li>Tung OOCL scholarship, 2015</li>
<li>Special Grade Scholarship, 2013</li>
<li>University-level Outstanding Students, 2013</li>
</ul>

<p><strong><font color="#4590a3" size="4px">Competitions</font></strong></p>
<ul>
<li><a href="http://ms-multimedia-challenge.com/2016/leaderboard">MSR-VTT Challenge</a> (video captioning): ranked 4th in human evaluation and ranked 5th in the automatic evaluation metrics (<b>Team leader</b>), 2016</li>
<li><b>Top 10%</b> in Kaggle Competition of Right Whale Recognition, 2016</li>
<li>Second Prize in DataCastle Competition of the Verification Code Recognition, 2016</li>
<li>Second Prize (<b>National-level</b>) in China Graduate	Student Mathematical Contest in Modeling, 2015</li>
<li>MCM/ICM -- Honorable Mention, 2012</li>
<li>First Prize (<b>National-level</b>) in Electrical Engineering Mathematical Contest in Modeling, 2012</li>
<li>First Prize (<b>National-level</b>) in China Undergraduate Mathematical Contest in Modeling, 2011</li>
</ul>

<p><strong><font color="#4590a3" size="4px">Teaching Assistant</font></strong></p>
<ul>
<li> 2015.9- 2016.1, Fudan University, COMP120008.02, C++ language programming</li>
	
</ul>

    </div>

    
  </div>
</article>

<div align="center">
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=260&t=tt&d=2Z8ppZasL5V6tY4NnAhjNERVHOH_c-p68ETGj3Efkbw&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353'></script>
</div>

    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2019 Zhiqiang Shen. All rights reserved<br>
      (Last update: Apr. 6, 2019.)<br>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>


<script src="script.js"></script>

</body>
</html>

